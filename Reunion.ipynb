{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    conn = psycopg2.connect(\n",
        "        dbname=\"Reunion\",\n",
        "        user=\"postgres\",\n",
        "        password=\"Chethu@1999\",\n",
        "        host=\"localhost\",\n",
        "        port=\"5433\"\n",
        "    )\n",
        "    print(\"Connection successful\")\n",
        "\n",
        "    # Create a cursor object\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # Create tables\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS CustomerDim (\n",
        "        customer_id SERIAL PRIMARY KEY,\n",
        "        name TEXT,\n",
        "        email TEXT,\n",
        "        contact_number TEXT,\n",
        "        address TEXT,\n",
        "        registration_date DATE,\n",
        "        is_active BOOLEAN\n",
        "    )''')\n",
        "\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS ProductDim (\n",
        "        product_id SERIAL PRIMARY KEY,\n",
        "        product_name TEXT,\n",
        "        category TEXT,\n",
        "        brand TEXT,\n",
        "        description TEXT\n",
        "    )''')\n",
        "\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS ProductVariantDim (\n",
        "        product_variant_id SERIAL PRIMARY KEY,\n",
        "        product_id INTEGER REFERENCES ProductDim(product_id),\n",
        "        variant_name TEXT,\n",
        "        launch_date DATE,\n",
        "        discontinuation_date DATE\n",
        "    )''')\n",
        "\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS PriceHistoryDim (\n",
        "        price_id SERIAL PRIMARY KEY,\n",
        "        product_variant_id INTEGER REFERENCES ProductVariantDim(product_variant_id),\n",
        "        price REAL,\n",
        "        effective_date DATE,\n",
        "        end_date DATE\n",
        "    )''')\n",
        "\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS OrderFact (\n",
        "        order_id SERIAL PRIMARY KEY,\n",
        "        customer_id INTEGER REFERENCES CustomerDim(customer_id),\n",
        "        product_variant_id INTEGER REFERENCES ProductVariantDim(product_variant_id),\n",
        "        order_date DATE,\n",
        "        quantity INTEGER,\n",
        "        total_price REAL\n",
        "    )''')\n",
        "\n",
        "    # Sample data\n",
        "    customer_names = ['John', 'Sirisha', 'Arpitha', 'Bhargav', 'Charan']\n",
        "    emails = ['john@example.com', 'Sirisha@example.com', 'Arpitha@example.com', 'Bhargav@example.com', 'Charan@example.com']\n",
        "    addresses = ['123 Goa', '456 Mangaluru', '789 Ooty', '101 Pondicherry', '202 Bangalore']\n",
        "    products = ['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Smartwatch']\n",
        "    categories = ['Electronics', 'Electronics', 'Electronics', 'Accessories', 'Electronics']\n",
        "    brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
        "\n",
        "    # Insert customers\n",
        "    for i in range(10):\n",
        "        name = random.choice(customer_names)\n",
        "        email = random.choice(emails)\n",
        "        contact_number = f'+1{random.randint(1000000000, 9999999999)}'\n",
        "        address = random.choice(addresses)\n",
        "        registration_date = datetime.now() - timedelta(days=random.randint(30, 730))\n",
        "        is_active = random.choice([True, False])\n",
        "        cur.execute('INSERT INTO CustomerDim (name, email, contact_number, address, registration_date, is_active) VALUES (%s, %s, %s, %s, %s, %s)',\n",
        "                    (name, email, contact_number, address, registration_date, is_active))\n",
        "\n",
        "    # Insert products and product variants\n",
        "    product_ids = []\n",
        "    for i in range(10):\n",
        "        product_name = products[i % len(products)]\n",
        "        category = categories[i % len(categories)]\n",
        "        brand = brands[i % len(brands)]\n",
        "        description = f'Description for {product_name}'\n",
        "        cur.execute('INSERT INTO ProductDim (product_name, category, brand, description) VALUES (%s, %s, %s, %s) RETURNING product_id',\n",
        "                    (product_name, category, brand, description))\n",
        "        product_id = cur.fetchone()[0]\n",
        "        product_ids.append(product_id)\n",
        "        for j in range(random.randint(1, 3)):\n",
        "            variant_name = f'{product_name} Variant {j+1}'\n",
        "            launch_date = datetime.now() - timedelta(days=random.randint(30, 365))\n",
        "            discontinuation_date = launch_date + timedelta(days=random.randint(30, 365)) if random.choice([True, False]) else None\n",
        "            cur.execute('INSERT INTO ProductVariantDim (product_id, variant_name, launch_date, discontinuation_date) VALUES (%s, %s, %s, %s) RETURNING product_variant_id',\n",
        "                        (product_id, variant_name, launch_date, discontinuation_date))\n",
        "            product_variant_id = cur.fetchone()[0]\n",
        "            for k in range(random.randint(1, 5)):\n",
        "                price = round(random.uniform(50, 1000), 2)\n",
        "                effective_date = launch_date + timedelta(days=random.randint(0, 180))\n",
        "                end_date = effective_date + timedelta(days=random.randint(30, 180)) if random.choice([True, False]) else None\n",
        "                cur.execute('INSERT INTO PriceHistoryDim (product_variant_id, price, effective_date, end_date) VALUES (%s, %s, %s, %s)',\n",
        "                            (product_variant_id, price, effective_date, end_date))\n",
        "\n",
        "    # Insert orders\n",
        "    for i in range(100):\n",
        "        customer_id = random.randint(1, 10)\n",
        "        product_variant_id = random.randint(1, len(product_ids) * 3)\n",
        "        order_date = datetime.now() - timedelta(days=random.randint(0, 730))\n",
        "        quantity = random.randint(1, 5)\n",
        "        total_price = round(quantity * random.uniform(50, 1000), 2)\n",
        "        cur.execute('INSERT INTO OrderFact (customer_id, product_variant_id, order_date, quantity, total_price) VALUES (%s, %s, %s, %s, %s)',\n",
        "                    (customer_id, product_variant_id, order_date, quantity, total_price))\n",
        "\n",
        "    # Commit and close connection\n",
        "    conn.commit()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "except psycopg2.OperationalError as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Rollback in case of any other error\n",
        "    conn.rollback()\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    if cur:\n",
        "        cur.close()\n",
        "    if conn:\n",
        "        conn.close()\n"
      ],
      "metadata": {
        "id": "OgBTCAfcdnFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZFEwzXd-iIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 2"
      ],
      "metadata": {
        "id": "NDlM-cSq-nHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 1. Retrieve the top 5 customers who have made the highest average order amounts in the last 6 months.\n",
        "\n",
        " SELECT\n",
        "    c.customer_id,\n",
        "    c.name,\n",
        "    AVG(o.total_price) AS avg_order_amount\n",
        "FROM\n",
        "    CustomerDim c\n",
        "JOIN\n",
        "    OrderFact o ON c.customer_id = o.customer_id\n",
        "WHERE\n",
        "    o.order_date >= CURRENT_DATE - INTERVAL '6 months'\n",
        "GROUP BY\n",
        "    c.customer_id, c.name\n",
        "ORDER BY\n",
        "    avg_order_amount DESC\n",
        "LIMIT 5;\n"
      ],
      "metadata": {
        "id": "OfwO1bxM-hJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Retrieve the list of customers whose order value is lower this year as compared to the previous year\n",
        "\n",
        "WITH yearly_order_totals AS (\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        EXTRACT(YEAR FROM order_date) AS order_year,\n",
        "        SUM(total_price) AS total_order_value\n",
        "    FROM\n",
        "        OrderFact\n",
        "    GROUP BY\n",
        "        customer_id, order_year\n",
        ")\n",
        "SELECT\n",
        "    current_year.customer_id,\n",
        "    c.name,\n",
        "    current_year.total_order_value AS current_year_value,\n",
        "    previous_year.total_order_value AS previous_year_value\n",
        "FROM\n",
        "    yearly_order_totals current_year\n",
        "JOIN\n",
        "    yearly_order_totals previous_year ON current_year.customer_id = previous_year.customer_id\n",
        "JOIN\n",
        "    CustomerDim c ON current_year.customer_id = c.customer_id\n",
        "WHERE\n",
        "    current_year.order_year = EXTRACT(YEAR FROM CURRENT_DATE)\n",
        "    AND previous_year.order_year = EXTRACT(YEAR FROM CURRENT_DATE) - 1\n",
        "    AND current_year.total_order_value < previous_year.total_order_value;\n",
        "\n"
      ],
      "metadata": {
        "id": "TZFoppYH-g_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a table showing cumulative purchase by a particular customer. Show the breakup of cumulative purchases by product category\n",
        "SELECT\n",
        "    c.customer_id,\n",
        "    c.name,\n",
        "    p.category,\n",
        "    SUM(o.total_price) AS cumulative_purchase\n",
        "FROM\n",
        "    OrderFact o\n",
        "JOIN\n",
        "    ProductVariantDim pv ON o.product_variant_id = pv.product_variant_id\n",
        "JOIN\n",
        "    ProductDim p ON pv.product_id = p.product_id\n",
        "JOIN\n",
        "    CustomerDim c ON o.customer_id = c.customer_id\n",
        "GROUP BY\n",
        "    c.customer_id, c.name, p.category\n",
        "ORDER BY\n",
        "    c.customer_id, p.category;\n"
      ],
      "metadata": {
        "id": "sILvEHpc--L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Retrieve the list of top 5 selling products. Further bifurcate the sales by product variants\n",
        "WITH product_sales AS (\n",
        "    SELECT\n",
        "        p.product_id,\n",
        "        p.product_name,\n",
        "        pv.product_variant_id,\n",
        "        pv.variant_name,\n",
        "        SUM(o.quantity) AS total_quantity_sold\n",
        "    FROM\n",
        "        OrderFact o\n",
        "    JOIN\n",
        "        ProductVariantDim pv ON o.product_variant_id = pv.product_variant_id\n",
        "    JOIN\n",
        "        ProductDim p ON pv.product_id = p.product_id\n",
        "    GROUP BY\n",
        "        p.product_id, p.product_name, pv.product_variant_id, pv.variant_name\n",
        ")\n",
        "SELECT\n",
        "    ps.product_id,\n",
        "    ps.product_name,\n",
        "    SUM(ps.total_quantity_sold) AS total_quantity_sold\n",
        "FROM\n",
        "    product_sales ps\n",
        "GROUP BY\n",
        "    ps.product_id, ps.product_name\n",
        "ORDER BY\n",
        "    total_quantity_sold DESC\n",
        "LIMIT 5;\n",
        "\n",
        "-- To bifurcate sales by product variants\n",
        "SELECT\n",
        "    ps.product_id,\n",
        "    ps.product_name,\n",
        "    ps.product_variant_id,\n",
        "    ps.variant_name,\n",
        "    ps.total_quantity_sold\n",
        "FROM\n",
        "    product_sales ps\n",
        "JOIN\n",
        "    (SELECT product_id FROM product_sales GROUP BY product_id ORDER BY SUM(total_quantity_sold) DESC LIMIT 5) top_products\n",
        "    ON ps.product_id = top_products.product_id\n",
        "ORDER BY\n",
        "    ps.product_id, ps.total_quantity_sold DESC;\n"
      ],
      "metadata": {
        "id": "6HIsWozJ_DaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "1. Top 5 Customers by Average Order Amount in the Last 6 Months:\n",
        "\n",
        "We calculate the average order amount for each customer for orders placed in the last 6 months and then sort the results in descending order, limiting to the top 5 customers.\n",
        "\n",
        "2. Customers with Lower Order Value This Year Compared to Last Year:\n",
        "\n",
        "Using a common table expression (CTE), we calculate the total order value per customer for each year. Then, we compare the total order value for the current year and the previous year and retrieve customers whose current year order value is lower.\n",
        "\n",
        "3. Cumulative Purchase by Customer and Product Category:\n",
        "\n",
        "This query joins the relevant tables and calculates the total purchase amount grouped by customer and product category, showing the cumulative purchase for each category per customer.\n",
        "\n",
        "4. Top 5 Selling Products and Their Variants:\n",
        "\n",
        "First, we calculate the total quantity sold for each product and identify the top 5 products. Then, we further bifurcate the sales by product variants for these top 5 products, displaying the total quantity sold for each variant."
      ],
      "metadata": {
        "id": "LPtcKqc5_Vmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3: ETL\n"
      ],
      "metadata": {
        "id": "xVjfURZvDSYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Load JSON data\n",
        "with open('nested_data.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize lists for normalized tables\n",
        "orchestra_data = []\n",
        "concert_data = []\n",
        "artist_data = []\n",
        "work_data = []\n",
        "performance_data = []\n",
        "\n",
        "# Extract and normalize data\n",
        "for concert in data['concerts']:\n",
        "    concert_id = concert['id']\n",
        "    concert_date = concert['date']\n",
        "    orchestra_name = concert['orchestra']\n",
        "    season = concert['season']\n",
        "\n",
        "    orchestra_data.append({\n",
        "        'orchestra_name': orchestra_name\n",
        "    })\n",
        "\n",
        "    concert_data.append({\n",
        "        'concert_id': concert_id,\n",
        "        'concert_date': concert_date,\n",
        "        'orchestra_name': orchestra_name,\n",
        "        'season': season\n",
        "    })\n",
        "\n",
        "    for performance in concert['programs']:\n",
        "        for work in performance['works']:\n",
        "            work_id = work['id']\n",
        "            work_title = work['title']\n",
        "            composer_name = work['composerName']\n",
        "\n",
        "            work_data.append({\n",
        "                'work_id': work_id,\n",
        "                'work_title': work_title,\n",
        "                'composer_name': composer_name\n",
        "            })\n",
        "\n",
        "            for artist in work['soloists']:\n",
        "                artist_id = artist['id']\n",
        "                artist_name = artist['name']\n",
        "                instrument = artist['instrument']\n",
        "\n",
        "                artist_data.append({\n",
        "                    'artist_id': artist_id,\n",
        "                    'artist_name': artist_name,\n",
        "                    'instrument': instrument\n",
        "                })\n",
        "\n",
        "                performance_data.append({\n",
        "                    'concert_id': concert_id,\n",
        "                    'work_id': work_id,\n",
        "                    'artist_id': artist_id\n",
        "                })\n",
        "\n",
        "# Convert lists to DataFrames\n",
        "orchestra_df = pd.DataFrame(orchestra_data).drop_duplicates().reset_index(drop=True)\n",
        "concert_df = pd.DataFrame(concert_data).drop_duplicates().reset_index(drop=True)\n",
        "artist_df = pd.DataFrame(artist_data).drop_duplicates().reset_index(drop=True)\n",
        "work_df = pd.DataFrame(work_data).drop_duplicates().reset_index(drop=True)\n",
        "performance_df = pd.DataFrame(performance_data).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "engine = create_engine('postgresql+psycopg2://postgres:Chethu@1999@localhost:5433/orchestra_db')\n",
        "\n",
        "# Write DataFrames to SQL\n",
        "orchestra_df.to_sql('orchestra', engine, if_exists='replace', index=False)\n",
        "concert_df.to_sql('concert', engine, if_exists='replace', index=False)\n",
        "artist_df.to_sql('artist', engine, if_exists='replace', index=False)\n",
        "work_df.to_sql('work', engine, if_exists='replace', index=False)\n",
        "performance_df.to_sql('performance', engine, if_exists='replace', index=False)\n"
      ],
      "metadata": {
        "id": "8F4q9DpKDL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Schema for Normalized Tables\n",
        "Here is the SQL code to create the tables and their relationships:\n"
      ],
      "metadata": {
        "id": "TfCFUhyKDhwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE Orchestra (\n",
        "    orchestra_name TEXT PRIMARY KEY\n",
        ");\n",
        "\n",
        "CREATE TABLE Concert (\n",
        "    concert_id SERIAL PRIMARY KEY,\n",
        "    concert_date DATE,\n",
        "    orchestra_name TEXT REFERENCES Orchestra(orchestra_name),\n",
        "    season TEXT\n",
        ");\n",
        "\n",
        "CREATE TABLE Artist (\n",
        "    artist_id SERIAL PRIMARY KEY,\n",
        "    artist_name TEXT,\n",
        "    instrument TEXT\n",
        ");\n",
        "\n",
        "CREATE TABLE Work (\n",
        "    work_id SERIAL PRIMARY KEY,\n",
        "    work_title TEXT,\n",
        "    composer_name TEXT\n",
        ");\n",
        "\n",
        "CREATE TABLE Performance (\n",
        "    performance_id SERIAL PRIMARY KEY,\n",
        "    concert_id INTEGER REFERENCES Concert(concert_id),\n",
        "    work_id INTEGER REFERENCES Work(work_id),\n",
        "    artist_id INTEGER REFERENCES Artist(artist_id)\n",
        ");\n"
      ],
      "metadata": {
        "id": "cunWdunkDj2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ERD\n",
        "Here's a description of the ERD:\n",
        "\n",
        "Orchestra: Contains information about the orchestra.\n",
        "Concert: Contains information about concerts, including a reference to the orchestra.\n",
        "Artist: Contains information about artists.\n",
        "Work: Contains information about musical works.\n",
        "Performance: Links concerts, works, and artists.\n",
        "The ERD would show these tables and the relationships:\n",
        "\n",
        "Concert has a foreign key referencing Orchestra.\n",
        "Performance has foreign keys referencing Concert, Work, and Artist.\n",
        "This provides a normalized schema suitable for efficient querying and analysis.\n"
      ],
      "metadata": {
        "id": "GPg8gLsTDofl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 4 Uploading JSON into AWS cloud\n"
      ],
      "metadata": {
        "id": "vyeWgIEz_-ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4XvX1GjAN7k",
        "outputId": "e156e01e-959f-456d-dd43-318977ee0921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.120-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.120 (from boto3)\n",
            "  Downloading botocore-1.34.120-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.120->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.120->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.120->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.120 botocore-1.34.120 jmespath-1.0.1 s3transfer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "s3=boto3.resource(\n",
        "    service_name='s3',\n",
        "    region_name='ap-south-1',\n",
        "    aws_access_key_id='AKIAZDXYVRUAUAFW4565',\n",
        "    aws_secret_access_key='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'\n",
        "    )"
      ],
      "metadata": {
        "id": "f3R8JM2eFEmb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bucet in s3.buckets.all():\n",
        "    print(bucet.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypS_XpIOCA_e",
        "outputId": "631cf654-55b0-4faa-a46c-9e8080595dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reunionfiles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s3.Bucket('reunionfiles').upload_file('/content/2018-2-4.json','2018-2-4.json')\n",
        "s3.Bucket('reunionfiles').upload_file('/content/2018-2-7.json','2018-2-7.json')\n",
        "s3.Bucket('reunionfiles').upload_file('/content/2018-2-8.json','2018-2-8.json')"
      ],
      "metadata": {
        "id": "SDkddJ0eAK_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4JUd8htEmOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY_EqkyPEotp",
        "outputId": "aa9a0392-c48d-406d-95df-a0efc5e6ff45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fbc8100cdef557caaf73c9a1621e741ac1936b758579524c2b997aab08efb858\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Ingest JSON Files\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "thGA3fb4EmLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env AWS_ACCESS_KEY_ID='AKIAZDXYVRUAUAFW4565'\n",
        "%env AWS_SECRET_ACCESS_KEY='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'\n",
        "%env AWS_DEFAULT_REGION='ap-south-1'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx1etMxlLKnS",
        "outputId": "55937c8a-2614-4998-96e8-6e2e9993d386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: AWS_ACCESS_KEY_ID='your_access_key_id'\n",
            "env: AWS_SECRET_ACCESS_KEY='your_secret_access_key'\n",
            "env: AWS_DEFAULT_REGION='ap-south-1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwfRkxEUK460",
        "outputId": "d872c70b-4e61-4348-b26d-2190339771ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=64e43d19398284dca4f9253dbb4e6bb09f448bfef290efac35d8b909fd0af683\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"fs.s3a.access.key\", \"AKIAZDXYVRUAUAFW4565\")\n",
        "spark.conf.set(\"fs.s3a.secret.key\", \"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\")"
      ],
      "metadata": {
        "id": "gjtQwrLTMUo7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"JsonIngestion\").getOrCreate()\n",
        "\n",
        "# Define the cloud storage path\n",
        "cloud_storage_path = \"s3a://reunionfiles/\"\n",
        "\n",
        "# Use the Autoloader to ingest JSON files\n",
        "df = (spark.readStream\n",
        "            .format(\"cloudFiles\")\n",
        "            .option(\"cloudFiles.format\", \"json\")\n",
        "            .load(cloud_storage_path))\n",
        "\n",
        "# Write the ingested data to a table\n",
        "(df.writeStream\n",
        "   .format(\"delta\")\n",
        "   .outputMode(\"append\")\n",
        "   .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\")\n",
        "   .table(\"ingested_json_table\"))\n",
        "\n",
        "# For automatic schema inference\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "id": "uwG9WMIANE-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 5"
      ],
      "metadata": {
        "id": "sySXX7Vd_zXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/Time Series.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Inspect the data\n",
        "print(df.head())\n",
        "\n",
        "# Assuming the Excel has columns: Bot, Activity, Start Time, End Time\n",
        "df['Start Time'] = pd.to_datetime(df['Start Time'])\n",
        "df['End Time'] = pd.to_datetime(df['End Time'])\n",
        "\n",
        "# Sort by Bot and Start Time\n",
        "df = df.sort_values(by=['Bot', 'Start Time'])\n",
        "\n",
        "# Function to merge intervals and aggregate activities\n",
        "def merge_intervals(df):\n",
        "    result = []\n",
        "    current_start = df.iloc[0]['Start Time']\n",
        "    current_end = df.iloc[0]['End Time']\n",
        "    current_activities = [df.iloc[0]['Activity']]\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "        row = df.iloc[i]\n",
        "        if row['Start Time'] <= current_end:\n",
        "            current_end = max(current_end, row['End Time'])\n",
        "            current_activities.append(row['Activity'])\n",
        "        else:\n",
        "            result.append({'Bot': row['Bot'], 'Start': current_start, 'End': current_end, 'Activities': current_activities})\n",
        "            current_start = row['Start Time']\n",
        "            current_end = row['End Time']\n",
        "            current_activities = [row['Activity']]\n",
        "\n",
        "    result.append({'Bot': row['Bot'], 'Start': current_start, 'End': current_end, 'Activities': current_activities})\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "# Apply the function to each bot\n",
        "merged_df = df.groupby('Bot').apply(merge_intervals).reset_index(drop=True)\n",
        "\n",
        "# Display the result\n",
        "print(merged_df)\n",
        "\n",
        "# Save the result to a new Excel file\n",
        "merged_df.to_excel('/content/merged_work_periods.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "5SU6FIbWFEg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Load and Inspect Data: Load the Excel file into a DataFrame and inspect its structure.\n",
        "Sort Data: Sort the DataFrame by bot and start time to facilitate merging intervals.\n",
        "Merge Intervals: Define a function to merge overlapping intervals and aggregate activities.\n",
        "Apply Function: Apply the function to each bot using groupby.\n",
        "Output Result: Display the merged DataFrame and save it to a new Excel file.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hs_nZTLoFK0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ac8xgYxUFaHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SQL Solution\n",
        "To implement this in SQL, we would use window functions to identify continuous periods and aggregate activities. Below is a conceptual SQL solution, assuming you have loaded the data into a SQL table named bot_activities."
      ],
      "metadata": {
        "id": "zZRavQtDFaon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q-cqHB40FXBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WITH OrderedActivities AS (\n",
        "    SELECT\n",
        "        Bot,\n",
        "        Activity,\n",
        "        [Start Time],\n",
        "        [End Time],\n",
        "        LAG([End Time]) OVER (PARTITION BY Bot ORDER BY [Start Time]) AS PrevEndTime\n",
        "    FROM bot_activities\n",
        "),\n",
        "GroupedActivities AS (\n",
        "    SELECT\n",
        "        Bot,\n",
        "        [Start Time] AS PeriodStart,\n",
        "        [End Time] AS PeriodEnd,\n",
        "        Activity,\n",
        "        SUM(CASE WHEN [Start Time] <= PrevEndTime THEN 0 ELSE 1 END)\n",
        "            OVER (PARTITION BY Bot ORDER BY [Start Time] ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS PeriodGroup\n",
        "    FROM OrderedActivities\n",
        ")\n",
        "SELECT\n",
        "    Bot,\n",
        "    MIN(PeriodStart) AS [Start],\n",
        "    MAX(PeriodEnd) AS [End],\n",
        "    STRING_AGG(Activity, ', ') AS Activities\n",
        "FROM GroupedActivities\n",
        "GROUP BY Bot, PeriodGroup\n",
        "ORDER BY Bot, [Start];\n"
      ],
      "metadata": {
        "id": "s05f6cZFFONy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "OrderedActivities: Use LAG to get the end time of the previous activity for each bot.\n",
        "GroupedActivities: Assign a group number to each activity period. Increment the group number whenever there is a gap between the current start time and the previous end time.\n",
        "Final Query: Group by bot and period group to get continuous periods and aggregate activities using STRING_AGG."
      ],
      "metadata": {
        "id": "5D23GLLIFfMG"
      }
    }
  ]
}