# -*- coding: utf-8 -*-
"""Reunion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LJe-Id3siw6C0JuQ_Ofxf1a1RIwAm6eZ
"""

import psycopg2
import random
from datetime import datetime, timedelta

try:
    conn = psycopg2.connect(
        dbname="Reunion",
        user="postgres",
        password="Chethu@1999",
        host="localhost",
        port="5433"
    )
    print("Connection successful")

    # Create a cursor object
    cur = conn.cursor()

    # Create tables
    cur.execute('''
    CREATE TABLE IF NOT EXISTS CustomerDim (
        customer_id SERIAL PRIMARY KEY,
        name TEXT,
        email TEXT,
        contact_number TEXT,
        address TEXT,
        registration_date DATE,
        is_active BOOLEAN
    )''')

    cur.execute('''
    CREATE TABLE IF NOT EXISTS ProductDim (
        product_id SERIAL PRIMARY KEY,
        product_name TEXT,
        category TEXT,
        brand TEXT,
        description TEXT
    )''')

    cur.execute('''
    CREATE TABLE IF NOT EXISTS ProductVariantDim (
        product_variant_id SERIAL PRIMARY KEY,
        product_id INTEGER REFERENCES ProductDim(product_id),
        variant_name TEXT,
        launch_date DATE,
        discontinuation_date DATE
    )''')

    cur.execute('''
    CREATE TABLE IF NOT EXISTS PriceHistoryDim (
        price_id SERIAL PRIMARY KEY,
        product_variant_id INTEGER REFERENCES ProductVariantDim(product_variant_id),
        price REAL,
        effective_date DATE,
        end_date DATE
    )''')

    cur.execute('''
    CREATE TABLE IF NOT EXISTS OrderFact (
        order_id SERIAL PRIMARY KEY,
        customer_id INTEGER REFERENCES CustomerDim(customer_id),
        product_variant_id INTEGER REFERENCES ProductVariantDim(product_variant_id),
        order_date DATE,
        quantity INTEGER,
        total_price REAL
    )''')

    # Sample data
    customer_names = ['John', 'Sirisha', 'Arpitha', 'Bhargav', 'Charan']
    emails = ['john@example.com', 'Sirisha@example.com', 'Arpitha@example.com', 'Bhargav@example.com', 'Charan@example.com']
    addresses = ['123 Goa', '456 Mangaluru', '789 Ooty', '101 Pondicherry', '202 Bangalore']
    products = ['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Smartwatch']
    categories = ['Electronics', 'Electronics', 'Electronics', 'Accessories', 'Electronics']
    brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']

    # Insert customers
    for i in range(10):
        name = random.choice(customer_names)
        email = random.choice(emails)
        contact_number = f'+1{random.randint(1000000000, 9999999999)}'
        address = random.choice(addresses)
        registration_date = datetime.now() - timedelta(days=random.randint(30, 730))
        is_active = random.choice([True, False])
        cur.execute('INSERT INTO CustomerDim (name, email, contact_number, address, registration_date, is_active) VALUES (%s, %s, %s, %s, %s, %s)',
                    (name, email, contact_number, address, registration_date, is_active))

    # Insert products and product variants
    product_ids = []
    for i in range(10):
        product_name = products[i % len(products)]
        category = categories[i % len(categories)]
        brand = brands[i % len(brands)]
        description = f'Description for {product_name}'
        cur.execute('INSERT INTO ProductDim (product_name, category, brand, description) VALUES (%s, %s, %s, %s) RETURNING product_id',
                    (product_name, category, brand, description))
        product_id = cur.fetchone()[0]
        product_ids.append(product_id)
        for j in range(random.randint(1, 3)):
            variant_name = f'{product_name} Variant {j+1}'
            launch_date = datetime.now() - timedelta(days=random.randint(30, 365))
            discontinuation_date = launch_date + timedelta(days=random.randint(30, 365)) if random.choice([True, False]) else None
            cur.execute('INSERT INTO ProductVariantDim (product_id, variant_name, launch_date, discontinuation_date) VALUES (%s, %s, %s, %s) RETURNING product_variant_id',
                        (product_id, variant_name, launch_date, discontinuation_date))
            product_variant_id = cur.fetchone()[0]
            for k in range(random.randint(1, 5)):
                price = round(random.uniform(50, 1000), 2)
                effective_date = launch_date + timedelta(days=random.randint(0, 180))
                end_date = effective_date + timedelta(days=random.randint(30, 180)) if random.choice([True, False]) else None
                cur.execute('INSERT INTO PriceHistoryDim (product_variant_id, price, effective_date, end_date) VALUES (%s, %s, %s, %s)',
                            (product_variant_id, price, effective_date, end_date))

    # Insert orders
    for i in range(100):
        customer_id = random.randint(1, 10)
        product_variant_id = random.randint(1, len(product_ids) * 3)
        order_date = datetime.now() - timedelta(days=random.randint(0, 730))
        quantity = random.randint(1, 5)
        total_price = round(quantity * random.uniform(50, 1000), 2)
        cur.execute('INSERT INTO OrderFact (customer_id, product_variant_id, order_date, quantity, total_price) VALUES (%s, %s, %s, %s, %s)',
                    (customer_id, product_variant_id, order_date, quantity, total_price))

    # Commit and close connection
    conn.commit()
    cur.close()
    conn.close()

except psycopg2.OperationalError as e:
    print(f"Connection failed: {e}")

except Exception as e:
    # Rollback in case of any other error
    conn.rollback()
    print(f"An error occurred: {e}")
    if cur:
        cur.close()
    if conn:
        conn.close()



"""PROBLEM 2"""

# 1. Retrieve the top 5 customers who have made the highest average order amounts in the last 6 months.

 SELECT
    c.customer_id,
    c.name,
    AVG(o.total_price) AS avg_order_amount
FROM
    CustomerDim c
JOIN
    OrderFact o ON c.customer_id = o.customer_id
WHERE
    o.order_date >= CURRENT_DATE - INTERVAL '6 months'
GROUP BY
    c.customer_id, c.name
ORDER BY
    avg_order_amount DESC
LIMIT 5;

#2. Retrieve the list of customers whose order value is lower this year as compared to the previous year

WITH yearly_order_totals AS (
    SELECT
        customer_id,
        EXTRACT(YEAR FROM order_date) AS order_year,
        SUM(total_price) AS total_order_value
    FROM
        OrderFact
    GROUP BY
        customer_id, order_year
)
SELECT
    current_year.customer_id,
    c.name,
    current_year.total_order_value AS current_year_value,
    previous_year.total_order_value AS previous_year_value
FROM
    yearly_order_totals current_year
JOIN
    yearly_order_totals previous_year ON current_year.customer_id = previous_year.customer_id
JOIN
    CustomerDim c ON current_year.customer_id = c.customer_id
WHERE
    current_year.order_year = EXTRACT(YEAR FROM CURRENT_DATE)
    AND previous_year.order_year = EXTRACT(YEAR FROM CURRENT_DATE) - 1
    AND current_year.total_order_value < previous_year.total_order_value;

# 3. Create a table showing cumulative purchase by a particular customer. Show the breakup of cumulative purchases by product category
SELECT
    c.customer_id,
    c.name,
    p.category,
    SUM(o.total_price) AS cumulative_purchase
FROM
    OrderFact o
JOIN
    ProductVariantDim pv ON o.product_variant_id = pv.product_variant_id
JOIN
    ProductDim p ON pv.product_id = p.product_id
JOIN
    CustomerDim c ON o.customer_id = c.customer_id
GROUP BY
    c.customer_id, c.name, p.category
ORDER BY
    c.customer_id, p.category;

# 4. Retrieve the list of top 5 selling products. Further bifurcate the sales by product variants
WITH product_sales AS (
    SELECT
        p.product_id,
        p.product_name,
        pv.product_variant_id,
        pv.variant_name,
        SUM(o.quantity) AS total_quantity_sold
    FROM
        OrderFact o
    JOIN
        ProductVariantDim pv ON o.product_variant_id = pv.product_variant_id
    JOIN
        ProductDim p ON pv.product_id = p.product_id
    GROUP BY
        p.product_id, p.product_name, pv.product_variant_id, pv.variant_name
)
SELECT
    ps.product_id,
    ps.product_name,
    SUM(ps.total_quantity_sold) AS total_quantity_sold
FROM
    product_sales ps
GROUP BY
    ps.product_id, ps.product_name
ORDER BY
    total_quantity_sold DESC
LIMIT 5;

-- To bifurcate sales by product variants
SELECT
    ps.product_id,
    ps.product_name,
    ps.product_variant_id,
    ps.variant_name,
    ps.total_quantity_sold
FROM
    product_sales ps
JOIN
    (SELECT product_id FROM product_sales GROUP BY product_id ORDER BY SUM(total_quantity_sold) DESC LIMIT 5) top_products
    ON ps.product_id = top_products.product_id
ORDER BY
    ps.product_id, ps.total_quantity_sold DESC;

"""Explanation:
1. Top 5 Customers by Average Order Amount in the Last 6 Months:

We calculate the average order amount for each customer for orders placed in the last 6 months and then sort the results in descending order, limiting to the top 5 customers.

2. Customers with Lower Order Value This Year Compared to Last Year:

Using a common table expression (CTE), we calculate the total order value per customer for each year. Then, we compare the total order value for the current year and the previous year and retrieve customers whose current year order value is lower.

3. Cumulative Purchase by Customer and Product Category:

This query joins the relevant tables and calculates the total purchase amount grouped by customer and product category, showing the cumulative purchase for each category per customer.

4. Top 5 Selling Products and Their Variants:

First, we calculate the total quantity sold for each product and identify the top 5 products. Then, we further bifurcate the sales by product variants for these top 5 products, displaying the total quantity sold for each variant.

Problem 3: ETL
"""

import json
import pandas as pd
from sqlalchemy import create_engine

# Load JSON data
with open('nested_data.json') as f:
    data = json.load(f)

# Initialize lists for normalized tables
orchestra_data = []
concert_data = []
artist_data = []
work_data = []
performance_data = []

# Extract and normalize data
for concert in data['concerts']:
    concert_id = concert['id']
    concert_date = concert['date']
    orchestra_name = concert['orchestra']
    season = concert['season']

    orchestra_data.append({
        'orchestra_name': orchestra_name
    })

    concert_data.append({
        'concert_id': concert_id,
        'concert_date': concert_date,
        'orchestra_name': orchestra_name,
        'season': season
    })

    for performance in concert['programs']:
        for work in performance['works']:
            work_id = work['id']
            work_title = work['title']
            composer_name = work['composerName']

            work_data.append({
                'work_id': work_id,
                'work_title': work_title,
                'composer_name': composer_name
            })

            for artist in work['soloists']:
                artist_id = artist['id']
                artist_name = artist['name']
                instrument = artist['instrument']

                artist_data.append({
                    'artist_id': artist_id,
                    'artist_name': artist_name,
                    'instrument': instrument
                })

                performance_data.append({
                    'concert_id': concert_id,
                    'work_id': work_id,
                    'artist_id': artist_id
                })

# Convert lists to DataFrames
orchestra_df = pd.DataFrame(orchestra_data).drop_duplicates().reset_index(drop=True)
concert_df = pd.DataFrame(concert_data).drop_duplicates().reset_index(drop=True)
artist_df = pd.DataFrame(artist_data).drop_duplicates().reset_index(drop=True)
work_df = pd.DataFrame(work_data).drop_duplicates().reset_index(drop=True)
performance_df = pd.DataFrame(performance_data).drop_duplicates().reset_index(drop=True)

# Create SQLAlchemy engine
engine = create_engine('postgresql+psycopg2://postgres:Chethu@1999@localhost:5433/orchestra_db')

# Write DataFrames to SQL
orchestra_df.to_sql('orchestra', engine, if_exists='replace', index=False)
concert_df.to_sql('concert', engine, if_exists='replace', index=False)
artist_df.to_sql('artist', engine, if_exists='replace', index=False)
work_df.to_sql('work', engine, if_exists='replace', index=False)
performance_df.to_sql('performance', engine, if_exists='replace', index=False)

"""SQL Schema for Normalized Tables
Here is the SQL code to create the tables and their relationships:

"""

CREATE TABLE Orchestra (
    orchestra_name TEXT PRIMARY KEY
);

CREATE TABLE Concert (
    concert_id SERIAL PRIMARY KEY,
    concert_date DATE,
    orchestra_name TEXT REFERENCES Orchestra(orchestra_name),
    season TEXT
);

CREATE TABLE Artist (
    artist_id SERIAL PRIMARY KEY,
    artist_name TEXT,
    instrument TEXT
);

CREATE TABLE Work (
    work_id SERIAL PRIMARY KEY,
    work_title TEXT,
    composer_name TEXT
);

CREATE TABLE Performance (
    performance_id SERIAL PRIMARY KEY,
    concert_id INTEGER REFERENCES Concert(concert_id),
    work_id INTEGER REFERENCES Work(work_id),
    artist_id INTEGER REFERENCES Artist(artist_id)
);

"""ERD
Here's a description of the ERD:

Orchestra: Contains information about the orchestra.
Concert: Contains information about concerts, including a reference to the orchestra.
Artist: Contains information about artists.
Work: Contains information about musical works.
Performance: Links concerts, works, and artists.
The ERD would show these tables and the relationships:

Concert has a foreign key referencing Orchestra.
Performance has foreign keys referencing Concert, Work, and Artist.
This provides a normalized schema suitable for efficient querying and analysis.

problem 4 Uploading JSON into AWS cloud
"""

pip install boto3

import pandas as pd
import boto3
import json

s3=boto3.resource(
    service_name='s3',
    region_name='ap-south-1',
    aws_access_key_id='AKIAZDXYVRUAUAFW4565',
    aws_secret_access_key='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'
    )

for bucet in s3.buckets.all():
    print(bucet.name)

s3.Bucket('reunionfiles').upload_file('/content/2018-2-4.json','2018-2-4.json')
s3.Bucket('reunionfiles').upload_file('/content/2018-2-7.json','2018-2-7.json')
s3.Bucket('reunionfiles').upload_file('/content/2018-2-8.json','2018-2-8.json')



pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Ingest JSON Files") \
    .getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
# %env AWS_ACCESS_KEY_ID='AKIAZDXYVRUAUAFW4565'
# %env AWS_SECRET_ACCESS_KEY='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'
# %env AWS_DEFAULT_REGION='ap-south-1'

pip install pyspark

spark.conf.set("fs.s3a.access.key", "AKIAZDXYVRUAUAFW4565")
spark.conf.set("fs.s3a.secret.key", "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")

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Create a Spark session
spark = SparkSession.builder.appName("JsonIngestion").getOrCreate()

# Define the cloud storage path
cloud_storage_path = "s3a://reunionfiles/"

# Use the Autoloader to ingest JSON files
df = (spark.readStream
            .format("cloudFiles")
            .option("cloudFiles.format", "json")
            .load(cloud_storage_path))

# Write the ingested data to a table
(df.writeStream
   .format("delta")
   .outputMode("append")
   .option("checkpointLocation", "/path/to/checkpoint/dir")
   .table("ingested_json_table"))

# For automatic schema inference
df.printSchema()

"""problem 5"""

import pandas as pd

# Load the Excel file
file_path = '/content/Time Series.xlsx'
df = pd.read_excel(file_path)

# Inspect the data
print(df.head())

# Assuming the Excel has columns: Bot, Activity, Start Time, End Time
df['Start Time'] = pd.to_datetime(df['Start Time'])
df['End Time'] = pd.to_datetime(df['End Time'])

# Sort by Bot and Start Time
df = df.sort_values(by=['Bot', 'Start Time'])

# Function to merge intervals and aggregate activities
def merge_intervals(df):
    result = []
    current_start = df.iloc[0]['Start Time']
    current_end = df.iloc[0]['End Time']
    current_activities = [df.iloc[0]['Activity']]

    for i in range(1, len(df)):
        row = df.iloc[i]
        if row['Start Time'] <= current_end:
            current_end = max(current_end, row['End Time'])
            current_activities.append(row['Activity'])
        else:
            result.append({'Bot': row['Bot'], 'Start': current_start, 'End': current_end, 'Activities': current_activities})
            current_start = row['Start Time']
            current_end = row['End Time']
            current_activities = [row['Activity']]

    result.append({'Bot': row['Bot'], 'Start': current_start, 'End': current_end, 'Activities': current_activities})
    return pd.DataFrame(result)

# Apply the function to each bot
merged_df = df.groupby('Bot').apply(merge_intervals).reset_index(drop=True)

# Display the result
print(merged_df)

# Save the result to a new Excel file
merged_df.to_excel('/content/merged_work_periods.xlsx', index=False)

"""Explanation:
Load and Inspect Data: Load the Excel file into a DataFrame and inspect its structure.
Sort Data: Sort the DataFrame by bot and start time to facilitate merging intervals.
Merge Intervals: Define a function to merge overlapping intervals and aggregate activities.
Apply Function: Apply the function to each bot using groupby.
Output Result: Display the merged DataFrame and save it to a new Excel file.




"""



"""
SQL Solution
To implement this in SQL, we would use window functions to identify continuous periods and aggregate activities. Below is a conceptual SQL solution, assuming you have loaded the data into a SQL table named bot_activities."""

WITH OrderedActivities AS (
    SELECT
        Bot,
        Activity,
        [Start Time],
        [End Time],
        LAG([End Time]) OVER (PARTITION BY Bot ORDER BY [Start Time]) AS PrevEndTime
    FROM bot_activities
),
GroupedActivities AS (
    SELECT
        Bot,
        [Start Time] AS PeriodStart,
        [End Time] AS PeriodEnd,
        Activity,
        SUM(CASE WHEN [Start Time] <= PrevEndTime THEN 0 ELSE 1 END)
            OVER (PARTITION BY Bot ORDER BY [Start Time] ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS PeriodGroup
    FROM OrderedActivities
)
SELECT
    Bot,
    MIN(PeriodStart) AS [Start],
    MAX(PeriodEnd) AS [End],
    STRING_AGG(Activity, ', ') AS Activities
FROM GroupedActivities
GROUP BY Bot, PeriodGroup
ORDER BY Bot, [Start];

"""Explanation:
OrderedActivities: Use LAG to get the end time of the previous activity for each bot.
GroupedActivities: Assign a group number to each activity period. Increment the group number whenever there is a gap between the current start time and the previous end time.
Final Query: Group by bot and period group to get continuous periods and aggregate activities using STRING_AGG.
"""